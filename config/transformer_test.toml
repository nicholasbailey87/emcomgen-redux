# Updated experimental settings, required to run perceiver models

name = "transformer_test" # experiment name - should be the same as file name
compile = true

[data]
n_workers = 4

[scheduler]
epochs = 100

[sender]
feature_model = "ViT2"
vision_dropout = 0.0
prototyper = "AttentionPrototyper"
language_model = "SenderTransformerLM"

[sender_feature_model]
d_model = 128 # Only applies to Transformer vision models
layers = 7 # Only applies to Transformer vision models
heads = 4 # Only applies to Transformer vision models
utility_tokens = 8 # Only applies to Transformer vision models

[sender_language_model]
message_length = 7 # Leave this in for Transformer agents to make sure it matches receiver model
token_embedding_size = 128 # Must be the same as d_model for transformer sender
d_model = 128
uniform_weight = 0.1
softmax_temperature = 16.0
exploration_temperature = 1.0
batch_norm_logits = false
dropout = 0.0
layers = 7
bidirectional = true
heads = 4 # Heads per layer - only applies to Transformer language models
utility_tokens = 8 # Only applies to Transformer language models

[receiver]
class = "Receiver"
feature_model = "ViT2"
comparer = "TransformerCrossAttentionComparer"
vision_dropout = 0.1

[receiver_feature_model]
d_model = 128 # Only applies to Transformer vision models
layers = 7 # Only applies to Transformer vision models
heads = 4 # Only applies to Transformer vision models
utility_tokens = 8 # Only applies to Transformer vision models

[receiver_comparer]
token_embedding_size = 500
d_model = 128
dropout = 0.0
layers = 7
bidirectional = true # No affect on transformer receivers
message_length = 7 # Only applies to Transformer comparers. Should be the same as sender_language_model.message_length
heads = 4 # Only applies to Transformer comparers
utility_tokens = 8 # Only applies to Transformer comparers