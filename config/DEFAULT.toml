# Default experimental settings, required to reproduce https://github.com/jayelm/emergent-generalization

name = "DEFAULT" # experiment name - should be the same as file name
resume = true
experiments_directory = "../exp"
cuda = true
compile = false
wandb = false
wandb_project_name = "cc"
use_lang = true
copy_receiver = false
receiver_only = false
share_feat_model = false
share_language_model = false
n_transformer_heads = 8
n_transformer_layers = 5
joint_training = false
joint_training_lambda = 1.0
reference_game = false
reference_game_xent = false
no_cross_eval = false
ignore_language = false
ignore_examples = false
debug = false
vis = false
receiver_reset_interval = 0.0
force_reference_game = false # Can be used in zero-shot eval
force_concept_game = false # Can be used in zero-shot eval
force_setref_game = false # Can be used in zero-shot eval
zero_shot_eval_epochs = 5
save_interval = 10

[data]
dataset = "../data/shapeworld"
ref_dataset="../data/shapeworld_ref"
load_shapeworld_into_memory = false
batch_size = 32
percent_novel = 1.0
n_examples = 20 # number of examples seen by agents, including distractors
n_sample = 2E5 # Number of samples for inspecting the language. Used in sample.py
n_workers = 0

[optimiser]
class = "AdamW"
accumulator_steps = 4
lr = 1e-4
weight_decay = 0.0
clip_grad_norm = 100.0
log_interval = 100

[scheduler]
epochs = 100

[sender]
class = "Sender"
feature_model = "Conv4"
vision_dropout = 0.1
prototyper = "AveragePrototyper"
language_model = "SenderGRULM"

[sender_feature_model]
embedding_size = 16 # Only applies to Transformer vision models
layers = 4 # Only applies to Transformer vision models
heads = 4 # Only applies to Transformer vision models
utility_tokens = 0 # Only applies to Transformer vision models

[sender_language_model]
token_embedding_size = 500
d_model = 1024
vocabulary = 14
message_length = 7 # max message length. note this includes sos/eos token, i.e. the true length is this - 2
softmax_temperature = 1.0
uniform_weight = 0.1
dropout = 0.1
layers = 1
bidirectional = false
confidence_based_exploration = true # Only applies to Transformer language models
heads = 4 # Heads per layer - only applies to Transformer language models
utility_tokens = 4 # Only applies to Transformer language models

[receiver]
class = "Receiver"
feature_model = "Conv4"
comparer = "BilinearGRUComparer"
vision_dropout = 0.1

[receiver_comparer]
token_embedding_size = 500
d_model = 1024
dropout = 0.1
layers = 1
bidirectional = false
message_length = 7 # Only applies to Transformer comparers. Should be the same as sender_language_model.message_length
heads = 4 # Only applies to Transformer comparers
utility_tokens = 0 # Only applies to Transformer comparers

# ====== Defaults for the CUB birds data set below here ======

[birds.data]
dataset = "../data/cub"
ref_dataset = "../data/cub"
batch_size = 16
n_workers = 4
n_examples = 10 # number of examples seen by agents, including distractors

[birds.optimiser]
accumulator_steps = 1

[birds.sender]
feature_model = "ResNet18"

[birds.sender_language_model]
vocabulary = 20
message_length = 10 # max message length. note this includes sos/eos token, i.e. the true length is this - 2

[birds.receiver]
feature_model = "ResNet18"

[birds.receiver_comparer]
message_length = 10 # Only applies to Transformer comparers. Should be the same as sender_language_model.message_length